{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_states = 2\n",
    "\n",
    "model = hmm.CategoricalHMM(n_components=hmm_states)\n",
    "model.emissionprob_ = np.array([[0.1,0.9],[0.9,0.1]])\n",
    "model.transmat_ = np.array([[0.9,0.1],[0.1,0.9]])\n",
    "model.startprob_ = np.array([0.8,0.2])\n",
    "data,states = model.sample(n_samples = 1000,random_state=28)\n",
    "data = np.squeeze(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# constant n_obs\n",
    "# n_obs = 1000\n",
    "# # Set p(H) apriori for simulation as a biased coin.\n",
    "# p_h = 0.8\n",
    "\n",
    "# # Model experiment as a single biased coin flipped 1000 times.\n",
    "# data = np.random.binomial(1, p_h, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_features = 2\n",
    "n_obs = data.shape[0]\n",
    "# emission = np.random.rand(n_obs,n_states)\n",
    "# emission = emission/np.tile(np.expand_dims(np.sum(emission,axis=1),axis=1),(1,2))\n",
    "\n",
    "emission = np.random.rand(n_states,n_features)\n",
    "emission = emission/np.tile(np.expand_dims(np.sum(emission,axis=1),axis=1),(1,2))\n",
    "# emission = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "\n",
    "transition =  np.random.rand(n_states,n_states)\n",
    "transition = transition/np.tile(np.expand_dims(np.sum(transition,axis=1),axis=1),(1,2))\n",
    "transition = np.array([[0.92,0.08],[0.1,0.9]])\n",
    "\n",
    "\n",
    "forward = np.random.rand(n_obs,n_states)\n",
    "scale_factors = np.zeros((n_obs))\n",
    "forward_hat = np.zeros((n_obs,n_states))\n",
    "\n",
    "backward = np.random.rand(n_obs,n_states)\n",
    "backward_hat = np.zeros((n_obs,n_states))\n",
    "\n",
    "init_prob = np.array([0.5,0.5])\n",
    "\n",
    "\n",
    "p_old = -10000\n",
    "tol = 0.0001\n",
    "max_iter = 10\n",
    "\n",
    "mu = np.random.rand(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission is [[0.52895737 0.47104263]\n",
      " [0.30210931 0.69789069]] and transition is [[0.92 0.08]\n",
      " [0.1  0.9 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'emission is {emission} and transition is {transition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is:-660.4496346855619,transition is [[0.9390847  0.0609153 ]\n",
      " [0.09555603 0.90444397]]\n",
      "p is:-588.2384251946714,transition is [[0.96653978 0.03346022]\n",
      " [0.07362996 0.92637004]]\n",
      "p is:-564.173172733355,transition is [[0.98683269 0.01316731]\n",
      " [0.05871576 0.94128424]]\n",
      "p is:-579.1065309313738,transition is [[0.99565698 0.00434302]\n",
      " [0.0826504  0.9173496 ]]\n",
      "p is:-604.6590225874478,transition is [[0.99896394 0.00103606]\n",
      " [0.26350603 0.73649397]]\n",
      "p is:-689.1666118061696,transition is [[9.99888786e-01 1.11214197e-04]\n",
      " [8.55705920e-01 1.44294080e-01]]\n",
      "p is:-831.4632957829967,transition is [[9.99999937e-01 6.33693493e-08]\n",
      " [9.99708342e-01 2.91657760e-04]]\n",
      "p is:-698.6505182034773,transition is [[1.00000000e+00 2.27361753e-11]\n",
      " [9.99999510e-01 4.89794627e-07]]\n",
      "p is:-691.9310064762531,transition is [[1.00000000e+00 8.09011703e-15]\n",
      " [9.99999999e-01 8.28998526e-10]]\n",
      "p is:-691.9309038649061,transition is [[1.00000000e+00 2.87866451e-18]\n",
      " [1.00000000e+00 1.40311846e-12]]\n"
     ]
    }
   ],
   "source": [
    "for ite in range(max_iter):\n",
    "    forward[0,:] = init_prob * emission[:,data[0]]\n",
    "    scale_factors[0] = np.sum(forward[0,:])\n",
    "    forward_hat[0,:] = forward[0,:]/scale_factors[0]\n",
    "    \n",
    "    for t in range(n_obs-1):\n",
    "        temp = np.matmul(forward_hat[t,:] ,np.transpose(transition)) * emission[:,data[t+1]]\n",
    "        scale_factors[t+1] = np.sum(temp)\n",
    "        forward_hat[t+1,:] = temp/scale_factors[t+1]\n",
    "        # print(f'temp is {temp} and the scale factor is {scale_factors[t+1]} and the forward_hat is {forward_hat[t+1]}')\n",
    "\n",
    "        forward[t+1,:] = forward_hat[t+1,:]*np.prod(scale_factors[0:t+1])\n",
    "\n",
    "    backward[-1,:] = 1\n",
    "    backward_hat[-1,:] = backward[-1,:]\n",
    "    for t in reversed(range(n_obs-1)):\n",
    "        temp = np.matmul(backward_hat[t+1,:]*emission[:,data[t+1]],transition)\n",
    "        backward_hat[t,:] = temp/scale_factors[t+1]\n",
    "        backward[t,:] = backward_hat[t,:]*np.prod(scale_factors[t+1:-1])\n",
    "\n",
    "\n",
    "    a = np.zeros((n_obs,n_states))\n",
    "    b = np.zeros((n_obs,n_states,n_states))\n",
    "    for i in range(n_obs):\n",
    "        for j in range(n_states):\n",
    "            a[i,j] = forward_hat[i,j]*backward_hat[i,j]\n",
    "    for t in range(n_obs-1):\n",
    "        for i in range(n_states):\n",
    "            for j in range(n_states):\n",
    "                b[t,i,j] = scale_factors[t+1]*forward_hat[t,i]*backward_hat[t+1,j]*transition[i,j]*emission[j,data[t+1]]\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            transition[i,j] = np.sum(b[0:-1,i,j])/np.sum(b[0:-1,i,:])\n",
    "            # print(np.sum(b[0:-1,i,j]),np.sum(b[0:-1,i,:]))\n",
    "\n",
    "    for i in range(n_states):\n",
    "        init_prob[i] = a[0,i]/np.sum(a[0,:])\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            emission[j,i] = np.sum(a[np.argwhere(data==i),j]) / np.sum(a[:,j])\n",
    "            \n",
    "    p = np.sum(np.log(scale_factors))\n",
    "    print(f'p is:{p},transition is {transition}')\n",
    "    # print(f'transition prob is: {transition}')\n",
    "    # print(f'emission prob is:{emission}')\n",
    "    if p>p_old and p - p_old < tol:\n",
    "        break\n",
    "    p_old = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.02748279 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 2.27307924\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 0.24238543 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.02748279 0.24238543 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.08678528]\n"
     ]
    }
   ],
   "source": [
    "print(b[0:-1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.69894213e-14, 1.00000000e+00],\n",
       "       [9.76588765e-20, 1.00000000e+00]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25860591, 0.74139409],\n",
       "       [0.94555583, 0.05444417]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = hmm.CategoricalHMM(n_components=n_states,emissionprob_prior=emission,transmat_prior=transition,startprob_prior=init_prob)\n",
    "model2 = hmm.CategoricalHMM(n_components=n_states,n_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely hidden states: [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "model2.fit(data.reshape(-1,1))\n",
    "hidden_states = model2.predict(data.reshape(-1,1))\n",
    "print(\"Most likely hidden states:\", hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.monitor_.converged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08646279, 0.91353721],\n",
       "       [0.87767051, 0.12232949]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.emissionprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88572501, 0.11427499],\n",
       "       [0.10842401, 0.89157599]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
