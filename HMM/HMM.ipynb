{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_states = 2\n",
    "\n",
    "model = hmm.CategoricalHMM(n_components=hmm_states)\n",
    "model.emissionprob_ = np.array([[0.1,0.9],[0.9,0.1]])\n",
    "model.transmat_ = np.array([[0.9,0.1],[0.1,0.9]])\n",
    "model.startprob_ = np.array([0.8,0.2])\n",
    "data,states = model.sample(n_samples = 1000,random_state=28)\n",
    "data = np.squeeze(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# constant n_obs\n",
    "# n_obs = 1000\n",
    "# # Set p(H) apriori for simulation as a biased coin.\n",
    "# p_h = 0.8\n",
    "\n",
    "# # Model experiment as a single biased coin flipped 1000 times.\n",
    "# data = np.random.binomial(1, p_h, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_features = 2\n",
    "n_obs = data.shape[0]\n",
    "# emission = np.random.rand(n_obs,n_states)\n",
    "# emission = emission/np.tile(np.expand_dims(np.sum(emission,axis=1),axis=1),(1,2))\n",
    "\n",
    "emission = np.random.rand(n_states,n_features)\n",
    "emission = emission/np.tile(np.expand_dims(np.sum(emission,axis=1),axis=1),(1,2))\n",
    "# emission = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "\n",
    "transition =  np.random.rand(n_states,n_states)\n",
    "transition = transition/np.tile(np.expand_dims(np.sum(transition,axis=1),axis=1),(1,2))\n",
    "transition = np.array([[0.76,0.24],[0.2,0.8]])\n",
    "\n",
    "\n",
    "forward = np.random.rand(n_obs,n_states)\n",
    "scale_factors = np.zeros((n_obs))\n",
    "forward_hat = np.zeros((n_obs,n_states))\n",
    "\n",
    "backward = np.random.rand(n_obs,n_states)\n",
    "backward_hat = np.zeros((n_obs,n_states))\n",
    "\n",
    "init_prob = np.array([0.5,0.5])\n",
    "\n",
    "\n",
    "p_old = -10000\n",
    "tol = 0.0001\n",
    "max_iter = 10\n",
    "\n",
    "mu = np.random.rand(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission is [[0.60368982 0.39631018]\n",
      " [0.82567216 0.17432784]] and transition is [[0.76 0.24]\n",
      " [0.2  0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'emission is {emission} and transition is {transition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is:-768.9427548163993,transition is [[0.73005689 0.26994311]\n",
      " [0.15229384 0.84770616]]\n",
      "p is:-638.9995894859331,transition is [[0.73713128 0.26286872]\n",
      " [0.07270504 0.92729496]]\n",
      "p is:-598.1261411699029,transition is [[0.78140559 0.21859441]\n",
      " [0.02091353 0.97908647]]\n",
      "p is:-599.6960880483696,transition is [[0.75316346 0.24683654]\n",
      " [0.00396405 0.99603595]]\n",
      "p is:-653.6271627974492,transition is [[4.01968463e-01 5.98031537e-01]\n",
      " [5.11937013e-04 9.99488063e-01]]\n",
      "p is:-839.7719869831195,transition is [[1.76298998e-02 9.82370100e-01]\n",
      " [1.13724770e-05 9.99988628e-01]]\n",
      "p is:-716.9145355505955,transition is [[2.91136749e-05 9.99970886e-01]\n",
      " [3.62928571e-09 9.99999996e-01]]\n",
      "p is:-691.9514207518048,transition is [[4.91736006e-08 9.99999951e-01]\n",
      " [1.29327305e-12 1.00000000e+00]]\n",
      "p is:-691.9309038354177,transition is [[8.32286082e-11 1.00000000e+00]\n",
      " [4.60178723e-16 1.00000000e+00]]\n",
      "p is:-691.9309038653856,transition is [[1.40868280e-13 1.00000000e+00]\n",
      " [1.63743016e-19 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for ite in range(max_iter):\n",
    "    forward[0,:] = init_prob * emission[:,data[0]]\n",
    "    scale_factors[0] = np.sum(forward[0,:])\n",
    "    forward_hat[0,:] = forward[0,:]/scale_factors[0]\n",
    "    \n",
    "    for t in range(n_obs-1):\n",
    "        temp = np.matmul(forward_hat[t,:] ,np.transpose(transition)) * emission[:,data[t+1]]\n",
    "        scale_factors[t+1] = np.sum(temp)\n",
    "        forward_hat[t+1,:] = temp/scale_factors[t+1]\n",
    "        # print(f'temp is {temp} and the scale factor is {scale_factors[t+1]} and the forward_hat is {forward_hat[t+1]}')\n",
    "\n",
    "        forward[t+1,:] = forward_hat[t+1,:]*np.prod(scale_factors[0:t+1])\n",
    "\n",
    "    backward[-1,:] = 1\n",
    "    backward_hat[-1,:] = backward[-1,:]\n",
    "    for t in reversed(range(n_obs-1)):\n",
    "        temp = np.matmul(backward_hat[t+1,:]*emission[:,data[t+1]],transition)\n",
    "        backward_hat[t,:] = temp/scale_factors[t+1]\n",
    "        backward[t,:] = backward_hat[t,:]*np.prod(scale_factors[t+1:-1])\n",
    "\n",
    "\n",
    "    a = np.zeros((n_obs,n_states))\n",
    "    b = np.zeros((n_obs,n_states,n_states))\n",
    "    for i in range(n_obs):\n",
    "        for j in range(n_states):\n",
    "            a[i,j] = forward_hat[i,j]*backward_hat[i,j]\n",
    "    for t in range(n_obs-1):\n",
    "        for i in range(n_states):\n",
    "            for j in range(n_states):\n",
    "                b[t,i,j] = scale_factors[t+1]*forward_hat[t,i]*backward_hat[t+1,j]*transition[i,j]*emission[j,data[t+1]]\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            transition[i,j] = np.sum(b[0:-1,i,j])/np.sum(b[0:-1,i,:])\n",
    "            # print(np.sum(b[0:-1,i,j]),np.sum(b[0:-1,i,:]))\n",
    "\n",
    "    for i in range(n_states):\n",
    "        init_prob[i] = a[0,i]/np.sum(a[0,:])\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            emission[j,i] = np.sum(a[np.argwhere(data==i),j]) / np.sum(a[:,j])\n",
    "            \n",
    "    p = np.sum(np.log(scale_factors))\n",
    "    print(f'p is:{p},transition is {transition}')\n",
    "    # print(f'transition prob is: {transition}')\n",
    "    # print(f'emission prob is:{emission}')\n",
    "    if p>p_old and p - p_old < tol:\n",
    "        break\n",
    "    p_old = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.02748279 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 2.27307924\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 0.24238543 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.02748279 2.27307924 0.02748279 0.24238543 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 2.27307924 0.02748279 0.24238543 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.02748279 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.02748279 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 2.27307924 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233 0.25773233\n",
      " 0.25773233 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 2.27307924 0.02748279 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.25773233 0.02748279 2.27307924 0.02748279 2.27307924 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233\n",
      " 0.02748279 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.02748279 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 2.27307924 0.25773233 0.25773233 0.02748279 2.27307924\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.02748279\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543 0.24238543\n",
      " 0.24238543 0.24238543 0.24238543 2.27307924 0.25773233 0.02748279\n",
      " 2.27307924 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233 0.25773233\n",
      " 0.25773233 0.25773233 0.25773233 0.02748279 2.27307924 0.25773233\n",
      " 0.25773233 0.25773233 0.08678528]\n"
     ]
    }
   ],
   "source": [
    "print(b[0:-1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.69894213e-14, 1.00000000e+00],\n",
       "       [9.76588765e-20, 1.00000000e+00]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25860591, 0.74139409],\n",
       "       [0.94555583, 0.05444417]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = hmm.CategoricalHMM(n_components=n_states,emissionprob_prior=emission,transmat_prior=transition,startprob_prior=init_prob)\n",
    "model2 = hmm.CategoricalHMM(n_components=n_states,n_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely hidden states: [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "model2.fit(data.reshape(-1,1))\n",
    "hidden_states = model2.predict(data.reshape(-1,1))\n",
    "print(\"Most likely hidden states:\", hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.monitor_.converged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08646279, 0.91353721],\n",
       "       [0.87767051, 0.12232949]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.emissionprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88572501, 0.11427499],\n",
       "       [0.10842401, 0.89157599]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
