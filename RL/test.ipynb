{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical,normal\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"Hopper-v5\")\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear = nn.Linear(11, 128)\n",
    "\n",
    "        self.action_mean = nn.Linear(128, 3)  # 3 actions, mean for each action\n",
    "        self.action_std = nn.Linear(128, 3)  # std for each action\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        # self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob =self.relu(self.action_mean(x))\n",
    "        action_std = self.softplus(self.action_std(x))\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, action_std\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear = nn.Linear(11, 128)\n",
    "        self.value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        state_values = self.value(x)\n",
    "        return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-2)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-2)\n",
    "# hyperparameters\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs_mean, probs_std= actor(state)\n",
    "    actions=[]\n",
    "    act_log_prob = []\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    for i in range(len(probs_mean)):\n",
    "        m = normal.Normal(probs_mean[i],probs_std[i])  # using normal distribution for sampling\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "        action = m.sample()\n",
    "        actions.append(action)\n",
    "        act_log_prob.append(m.log_prob(action))\n",
    "    actions = torch.tensor(actions)\n",
    "    logp = torch.sum(torch.stack(act_log_prob),dim=0)\n",
    "    return actions.numpy(),logp\n",
    "\n",
    "def get_value(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state_value = critic(state)\n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(rewards,acts,values,logp):\n",
    "    policy_losses = []\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns=[]\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, value, R in zip(logp, values,rewards):\n",
    "        advantage = R - value\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage.detach())\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "    policyloss = torch.stack(policy_losses).sum()\n",
    "    criticloss = torch.stack(value_losses).sum()\n",
    "    return policyloss, criticloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 26.67\n",
      "Episode 10\tLast reward: 13.96\n",
      "Episode 20\tLast reward: 71.39\n",
      "Episode 30\tLast reward: 7.53\n",
      "Episode 40\tLast reward: 24.47\n",
      "Episode 50\tLast reward: 12.54\n",
      "Episode 60\tLast reward: 68.34\n",
      "Episode 70\tLast reward: 54.28\n",
      "Episode 80\tLast reward: 6.59\n",
      "Episode 90\tLast reward: 11.88\n",
      "Episode 100\tLast reward: 71.34\n",
      "Episode 110\tLast reward: 43.71\n",
      "Episode 120\tLast reward: 19.53\n",
      "Episode 130\tLast reward: 14.58\n",
      "Episode 140\tLast reward: 38.33\n",
      "Episode 150\tLast reward: 23.09\n",
      "Episode 160\tLast reward: 10.28\n",
      "Episode 170\tLast reward: 78.81\n",
      "Episode 180\tLast reward: 34.53\n",
      "Episode 190\tLast reward: 184.09\n",
      "Episode 200\tLast reward: 73.63\n",
      "Episode 210\tLast reward: 36.32\n",
      "Episode 220\tLast reward: 22.18\n",
      "Episode 230\tLast reward: 12.79\n",
      "Episode 240\tLast reward: 31.68\n",
      "Episode 250\tLast reward: 181.46\n",
      "Episode 260\tLast reward: 31.80\n",
      "Episode 270\tLast reward: 34.89\n",
      "Episode 280\tLast reward: 104.77\n",
      "Episode 290\tLast reward: 34.81\n",
      "Episode 300\tLast reward: 31.82\n",
      "Episode 310\tLast reward: 30.21\n",
      "Episode 320\tLast reward: 47.94\n",
      "Episode 330\tLast reward: 26.89\n",
      "Episode 340\tLast reward: 38.39\n",
      "Episode 350\tLast reward: 81.93\n",
      "Episode 360\tLast reward: 39.28\n",
      "Episode 370\tLast reward: 20.79\n",
      "Episode 380\tLast reward: 34.25\n",
      "Episode 390\tLast reward: 26.93\n",
      "Episode 400\tLast reward: 70.36\n",
      "Episode 410\tLast reward: 43.54\n",
      "Episode 420\tLast reward: 69.25\n",
      "Episode 430\tLast reward: 25.06\n",
      "Episode 440\tLast reward: 18.77\n",
      "Episode 450\tLast reward: 12.19\n",
      "Episode 460\tLast reward: 14.52\n",
      "Episode 470\tLast reward: 44.82\n",
      "Episode 480\tLast reward: 18.35\n",
      "Episode 490\tLast reward: 9.67\n",
      "Episode 500\tLast reward: 7.12\n",
      "Episode 510\tLast reward: 53.04\n",
      "Episode 520\tLast reward: 40.10\n",
      "Episode 530\tLast reward: 21.14\n",
      "Episode 540\tLast reward: 86.36\n",
      "Episode 550\tLast reward: 20.25\n",
      "Episode 560\tLast reward: 38.28\n",
      "Episode 570\tLast reward: 85.22\n",
      "Episode 580\tLast reward: 62.68\n",
      "Episode 590\tLast reward: 56.08\n",
      "Episode 600\tLast reward: 24.54\n",
      "Episode 610\tLast reward: 12.94\n",
      "Episode 620\tLast reward: 12.09\n",
      "Episode 630\tLast reward: 15.16\n",
      "Episode 640\tLast reward: 86.99\n",
      "Episode 650\tLast reward: 26.89\n",
      "Episode 660\tLast reward: 9.12\n",
      "Episode 670\tLast reward: 9.80\n",
      "Episode 680\tLast reward: 7.92\n",
      "Episode 690\tLast reward: 17.02\n",
      "Episode 700\tLast reward: 16.62\n",
      "Episode 710\tLast reward: 9.38\n",
      "Episode 720\tLast reward: 23.68\n",
      "Episode 730\tLast reward: 25.91\n",
      "Episode 740\tLast reward: 12.58\n",
      "Episode 750\tLast reward: 9.81\n",
      "Episode 760\tLast reward: 5.42\n",
      "Episode 770\tLast reward: 16.00\n",
      "Episode 780\tLast reward: 14.53\n",
      "Episode 790\tLast reward: 91.11\n",
      "Episode 800\tLast reward: 44.55\n",
      "Episode 810\tLast reward: 11.04\n",
      "Episode 820\tLast reward: 32.63\n",
      "Episode 830\tLast reward: 41.57\n",
      "Episode 840\tLast reward: 109.01\n",
      "Episode 850\tLast reward: 78.28\n",
      "Episode 860\tLast reward: 34.27\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape ()) of distribution Normal(loc: nan, scale: nan) to satisfy the constraint Real(), but found invalid values:\nnan",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# for each episode, only run 9999 steps so that we don't\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# infinite loop while learning\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m10000\u001b[39m):\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# select action from policy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     action,logp = \u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     value = get_value(state)\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# take the action\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_action\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# create a categorical distribution over the list of probabilities of actions\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(probs_mean)):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     m = \u001b[43mnormal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprobs_std\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# using normal distribution for sampling\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# and sample an action using the distribution\u001b[39;00m\n\u001b[32m     11\u001b[39m     action = m.sample()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL/lib/python3.12/site-packages/torch/distributions/normal.py:59\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/RL/lib/python3.12/site-packages/torch/distributions/distribution.py:71\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter loc (Tensor of shape ()) of distribution Normal(loc: nan, scale: nan) to satisfy the constraint Real(), but found invalid values:\nnan"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "log_interval = 10\n",
    "render=False\n",
    "# run infinitely many episodes\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state, _ = env.reset()\n",
    "    episode_rewards = []\n",
    "    episode_acts = []\n",
    "    episode_values = []\n",
    "    episode_logp = []\n",
    "    # for each episode, only run 9999 steps so that we don't\n",
    "    # infinite loop while learning\n",
    "    for t in range(1, 10000):\n",
    "\n",
    "        # select action from policy\n",
    "        action,logp = get_action(state)\n",
    "        print(\"action\",action,\"logp\",logp)\n",
    "        value = get_value(state)\n",
    "        # take the action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        episode_acts.append(action)\n",
    "        episode_values.append(value)\n",
    "        episode_logp.append(logp)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # perform backprop\n",
    "    # reset gradients\n",
    "    actor_optimizer.zero_grad()\n",
    "    critic_optimizer.zero_grad()\n",
    "    actor_loss,critic_loss = compute_loss(episode_rewards, episode_acts, episode_values,episode_logp)\n",
    "    # print(\"actor_loss\",actor_loss,\"critic_loss\",critic_loss)\n",
    "    actor_loss.backward()\n",
    "    critic_loss.backward()\n",
    "\n",
    "    actor_optimizer.step()\n",
    "    critic_optimizer.step()\n",
    "    # log results\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}'.format(\n",
    "                i_episode, np.sum(episode_rewards)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch():\n",
    "#     # make some empty lists for logging.\n",
    "#     batch_states = []          # for observations\n",
    "#     batch_acts = []         # for actions\n",
    "#     batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "#     batch_rets = []         # for measuring episode returns\n",
    "#     batch_lens = []         # for measuring episode lengths\n",
    "#     batch_size = 10000      # number of steps to take in each epoch\n",
    "#     # reset episode-specific variables\n",
    "#     states = env.reset()       # first obs comes from starting distribution\n",
    "#     done = False            # signal from environment that episode is over\n",
    "#     epoch_rewards = []            # list for rewards accrued throughout ep\n",
    "\n",
    "\n",
    "#     # collect experience by acting in the environment with current policy\n",
    "#     while True:\n",
    "\n",
    "#         # save states\n",
    "#         batch_states.append(states.copy())\n",
    "\n",
    "#         # act in the environment\n",
    "#         act = get_action(torch.as_tensor(states, dtype=torch.float32))\n",
    "#         states, reward, terminated, truncated,_ = env.step(act)\n",
    "\n",
    "#         # save action, reward\n",
    "#         batch_acts.append(act)\n",
    "#         epoch_rewards.append(reward)\n",
    "\n",
    "#         if terminated or truncated:\n",
    "#             # if episode is over, record info about episode\n",
    "#             epoch_return, epoch_len = sum(epoch_rewards), len(epoch_rewards)\n",
    "#             batch_rets.append(ep_ret)\n",
    "#             batch_lens.append(ep_len)\n",
    "\n",
    "#             # the weight for each logprob(a|s) is R(tau)\n",
    "#             batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "#             # reset episode-specific variables\n",
    "#             obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "\n",
    "#             # end experience loop if we have enough of it\n",
    "#             if len(batch_obs) > batch_size:\n",
    "#                 break\n",
    "\n",
    "#     # take a single policy gradient update step\n",
    "#     actor_optimizer.zero_grad()\n",
    "#     batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "#                               act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "#                               weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "#                               )\n",
    "#     batch_loss.backward()\n",
    "#     critic_optimizer.step()\n",
    "#     return batch_loss, batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
