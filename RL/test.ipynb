{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical,normal\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"Hopper-v5\")\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear = nn.Linear(11, 128)\n",
    "\n",
    "        self.action_mean = nn.Linear(128, 3)  # 3 actions, mean for each action\n",
    "        self.action_std = nn.Linear(128, 3)  # std for each action\n",
    "        # self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.relu(self.action_mean(x))\n",
    "        action_std = F.softplus(self.action_std(x))\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, action_std\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear = nn.Linear(11, 128)\n",
    "        self.value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear(x))\n",
    "        state_values = self.value(x)\n",
    "        return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor().to(device)\n",
    "critic = Critic().to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-2)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-2)\n",
    "# hyperparameters\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs_mean, probs_std= actor_optimizer(state)\n",
    "    actions=[]\n",
    "    act_log_prob = []\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    for i in range(len(probs_mean)):\n",
    "        m = normal.Normal(probs_mean[i],probs_std[i])  # using normal distribution for sampling\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "        action = m.sample()\n",
    "        actions.append(action)\n",
    "        act_log_prob.append(m.log_prob(action))\n",
    "    actions = torch.tensor(actions)\n",
    "    logp = np.sum(act_log_prob)\n",
    "    return actions.numpy(),logp\n",
    "\n",
    "def get_value(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state_value = critic(state)\n",
    "    return state_value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(rewards,acts,values,logp):\n",
    "    policy_losses = []\n",
    "    \n",
    "    for r in rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    for (log_prob, value), R in zip(logp, values,rewards):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        loss = torch.stack(policy_losses).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "log_interval = 10\n",
    "render=False\n",
    "# run infinitely many episodes\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state, _ = env.reset()\n",
    "    episode_rewards = []\n",
    "    episode_acts = []\n",
    "    episode_values = []\n",
    "    episode_logp = []\n",
    "    # for each episode, only run 9999 steps so that we don't\n",
    "    # infinite loop while learning\n",
    "    for t in range(1, 10000):\n",
    "\n",
    "        # select action from policy\n",
    "        action,logp = get_action(state)\n",
    "        value = get_value(state)\n",
    "        # take the action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        episode_acts.append(action)\n",
    "        episode_values.append(value)\n",
    "        episode_logp.append(logp)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "\n",
    "    # perform backprop\n",
    "    compute_loss(episode_rewards, episode_acts, episode_values,episode_logp)\n",
    "    actor_optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    # log results\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                i_episode, ep_reward, running_reward))\n",
    "\n",
    "    # check if we have \"solved\" the cart pole problem\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    # make some empty lists for logging.\n",
    "    batch_states = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "    batch_size = 10000      # number of steps to take in each epoch\n",
    "    # reset episode-specific variables\n",
    "    states = env.reset()       # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    epoch_rewards = []            # list for rewards accrued throughout ep\n",
    "\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "\n",
    "        # save states\n",
    "        batch_states.append(states.copy())\n",
    "\n",
    "        # act in the environment\n",
    "        act = get_action(torch.as_tensor(states, dtype=torch.float32))\n",
    "        states, reward, terminated, truncated,_ = env.step(act)\n",
    "\n",
    "        # save action, reward\n",
    "        batch_acts.append(act)\n",
    "        epoch_rewards.append(reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            # if episode is over, record info about episode\n",
    "            epoch_return, epoch_len = sum(epoch_rewards), len(epoch_rewards)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "\n",
    "            # the weight for each logprob(a|s) is R(tau)\n",
    "            batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "            # reset episode-specific variables\n",
    "            obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "\n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "\n",
    "    # take a single policy gradient update step\n",
    "    actor_optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "    batch_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    return batch_loss, batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
